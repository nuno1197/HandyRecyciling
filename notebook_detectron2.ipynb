{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_767SpEGD2zJ"
      },
      "source": [
        "<img src=\"https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png\" width=\"500\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V8w1ew59buh"
      },
      "source": [
        "Now is a good time to confirm that we have the right versions of the libraries at our disposal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqCNglJXRro5",
        "outputId": "0aa2b469-8b0b-4feb-c2ee-959cda50ea73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Tue_May__3_19:00:59_Pacific_Daylight_Time_2022\n",
            "Cuda compilation tools, release 11.7, V11.7.64\n",
            "Build cuda_11.7.r11.7/compiler.31294372_0\n",
            "torch:  2.0 ; cuda:  cu117\n",
            "detectron2: 0.6\n"
          ]
        }
      ],
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DIEKfPKFmW54"
      },
      "outputs": [],
      "source": [
        "# COMMON LIBRARIES\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# DATA SET PREPARATION AND LOADING\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "# VISUALIZATION\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "\n",
        "# CONFIGURATION\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "# EVALUATION\n",
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "# TRAINING\n",
        "from detectron2.engine import DefaultTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFkJOTWvxu6G"
      },
      "source": [
        "## COCO Format Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoB31yi4AoYs"
      },
      "source": [
        "### Register"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HopUGOyW853G"
      },
      "source": [
        "When you use Detectron2, before you actually train the model you need to [register it](https://detectron2.readthedocs.io/en/latest/tutorials/datasets.html#register-a-coco-format-dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jntOI8GJG2ks"
      },
      "outputs": [],
      "source": [
        "# TRAIN SET\n",
        "TRAIN_DATA_SET_NAME = \"v1-train\"\n",
        "TRAIN_DATA_SET_IMAGES_DIR_PATH = \"./datasets/v1/train/\"\n",
        "TRAIN_DATA_SET_ANN_FILE_PATH = \"./datasets/v1/train/_annotations.coco.json\"\n",
        "\n",
        "register_coco_instances(\n",
        "    name=TRAIN_DATA_SET_NAME, \n",
        "    metadata={}, \n",
        "    json_file=TRAIN_DATA_SET_ANN_FILE_PATH, \n",
        "    image_root=TRAIN_DATA_SET_IMAGES_DIR_PATH\n",
        ")\n",
        "\n",
        "\n",
        "# VALID SET\n",
        "VALID_DATA_SET_NAME = \"v1-valid\"\n",
        "VALID_DATA_SET_IMAGES_DIR_PATH = \"./datasets/v1/valid/\"\n",
        "VALID_DATA_SET_ANN_FILE_PATH = \"./datasets/v1/valid/_annotations.coco.json\"\n",
        "\n",
        "register_coco_instances(\n",
        "    name=VALID_DATA_SET_NAME, \n",
        "    metadata={}, \n",
        "    json_file=VALID_DATA_SET_ANN_FILE_PATH, \n",
        "    image_root=VALID_DATA_SET_IMAGES_DIR_PATH\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOCY1UWNCtnq"
      },
      "source": [
        "We can now confirm that our custom dataset was correctly registered using [MetadataCatalog](https://detectron2.readthedocs.io/en/latest/modules/data.html#detectron2.data.MetadataCatalog)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR8ha4EHCkA-",
        "outputId": "6506603a-3742-43ee-af5d-7f842426d25d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['v1-train']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[\n",
        "    data_set\n",
        "    for data_set\n",
        "    in MetadataCatalog.list()\n",
        "    if data_set.startswith(\"v1-train\")\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['v1-valid']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[\n",
        "    data_set\n",
        "    for data_set\n",
        "    in MetadataCatalog.list()\n",
        "    if data_set.startswith(\"v1-valid\")\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GavGRHy2M7Hb"
      },
      "source": [
        "## Train Model Using Custom COCO Format Dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ3g-l56NMOY"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "krCm2L_lNC83"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "ARCHITECTURE = \"mask_rcnn_R_50_FPN_3x\"\n",
        "CONFIG_FILE_PATH = f\"COCO-InstanceSegmentation/{ARCHITECTURE}.yaml\"\n",
        "MAX_ITER = 25000\n",
        "EVAL_PERIOD = 200\n",
        "BASE_LR = 0.001\n",
        "NUM_CLASSES = 67\n",
        "\n",
        "\n",
        "# OUTPUT DIR\n",
        "OUTPUT_DIR_PATH = os.path.join(\n",
        "    \"v1-train\", \n",
        "    ARCHITECTURE, \n",
        "    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
        ")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lxQU8JrgOD73"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE_PATH))\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_FILE_PATH)\n",
        "#cfg.MODEL.WEIGHTS = \"./v1-train/mask_rcnn_R_50_FPN_3x/2023-08-08-11-01-05/model_final.pth\"\n",
        "cfg.DATASETS.TRAIN = (\"v1-train\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
        "cfg.TEST.EVAL_PERIOD = EVAL_PERIOD\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.SOLVER.IMS_PER_BATCH = 4\n",
        "cfg.INPUT.MASK_FORMAT='bitmask'\n",
        "cfg.SOLVER.BASE_LR = BASE_LR\n",
        "cfg.SOLVER.MAX_ITER = MAX_ITER\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES\n",
        "cfg.OUTPUT_DIR = OUTPUT_DIR_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch-_5aCuXWj9"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S8y2W2AQvJq",
        "outputId": "36fe2d26-1118-4748-fff1-18a86d873970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[08/09 00:07:47 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=68, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=268, bias=True)\n",
            "    )\n",
            "    (mask_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (mask_head): MaskRCNNConvUpsampleHead(\n",
            "      (mask_fcn1): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn2): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn3): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (mask_fcn4): Conv2d(\n",
            "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "      (deconv_relu): ReLU()\n",
            "      (predictor): Conv2d(256, 67, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/09 00:07:47 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[08/09 00:07:47 d2.data.datasets.coco]: \u001b[0mLoaded 910 images in COCO format from ./datasets/v1/train/_annotations.coco.json\n",
            "\u001b[32m[08/09 00:07:47 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 910 images left.\n",
            "\u001b[32m[08/09 00:07:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[08/09 00:07:47 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[08/09 00:07:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "\u001b[32m[08/09 00:07:47 d2.data.common]: \u001b[0mSerializing 910 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[08/09 00:07:47 d2.data.common]: \u001b[0mSerialized dataset takes 2.82 MiB\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/09 00:07:47 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
            "\u001b[32m[08/09 00:07:47 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (68, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (68,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (268, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (268,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (67, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (67,) in the model! You might want to double check if this is expected.\n",
            "Some model parameters or buffers are not found in the checkpoint:\n",
            "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
            "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[08/09 00:07:47 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[08/09 00:10:03 d2.utils.events]: \u001b[0m eta: 1 day, 22:34:01  iter: 19  total_loss: 5.869  loss_cls: 4.405  loss_box_reg: 0.7057  loss_mask: 0.6942  loss_rpn_cls: 0.03901  loss_rpn_loc: 0.0265    time: 6.6447  last_time: 5.8330  data_time: 0.1659  last_data_time: 0.0071   lr: 1.9981e-05  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:12:04 d2.utils.events]: \u001b[0m eta: 1 day, 20:11:57  iter: 39  total_loss: 5.456  loss_cls: 4.021  loss_box_reg: 0.671  loss_mask: 0.6912  loss_rpn_cls: 0.05591  loss_rpn_loc: 0.02282    time: 6.3325  last_time: 5.3720  data_time: 0.0318  last_data_time: 0.0058   lr: 3.9961e-05  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:14:24 d2.utils.events]: \u001b[0m eta: 1 day, 21:19:19  iter: 59  total_loss: 4.709  loss_cls: 3.206  loss_box_reg: 0.6841  loss_mask: 0.6802  loss_rpn_cls: 0.05116  loss_rpn_loc: 0.03188    time: 6.5559  last_time: 8.5301  data_time: 0.0436  last_data_time: 0.0090   lr: 5.9941e-05  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:16:33 d2.utils.events]: \u001b[0m eta: 1 day, 21:04:57  iter: 79  total_loss: 3.262  loss_cls: 1.755  loss_box_reg: 0.7013  loss_mask: 0.6691  loss_rpn_cls: 0.05206  loss_rpn_loc: 0.02406    time: 6.5274  last_time: 6.0220  data_time: 0.0351  last_data_time: 0.1096   lr: 7.9921e-05  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:18:48 d2.utils.events]: \u001b[0m eta: 1 day, 21:22:13  iter: 99  total_loss: 2.85  loss_cls: 1.34  loss_box_reg: 0.7974  loss_mask: 0.6445  loss_rpn_cls: 0.03232  loss_rpn_loc: 0.02054    time: 6.5723  last_time: 7.9923  data_time: 0.0451  last_data_time: 0.0060   lr: 9.9901e-05  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:20:53 d2.utils.events]: \u001b[0m eta: 1 day, 21:12:18  iter: 119  total_loss: 2.641  loss_cls: 1.222  loss_box_reg: 0.7308  loss_mask: 0.6398  loss_rpn_cls: 0.03824  loss_rpn_loc: 0.02354    time: 6.5182  last_time: 5.2970  data_time: 0.0199  last_data_time: 0.0046   lr: 0.00011988  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:22:57 d2.utils.events]: \u001b[0m eta: 1 day, 20:26:02  iter: 139  total_loss: 2.568  loss_cls: 1.138  loss_box_reg: 0.7649  loss_mask: 0.6095  loss_rpn_cls: 0.02696  loss_rpn_loc: 0.01938    time: 6.4752  last_time: 7.2500  data_time: 0.0254  last_data_time: 0.0047   lr: 0.00013986  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:25:10 d2.utils.events]: \u001b[0m eta: 1 day, 20:23:53  iter: 159  total_loss: 2.57  loss_cls: 1.129  loss_box_reg: 0.7725  loss_mask: 0.5792  loss_rpn_cls: 0.03041  loss_rpn_loc: 0.03337    time: 6.4948  last_time: 5.7838  data_time: 0.0531  last_data_time: 0.1592   lr: 0.00015984  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:27:14 d2.utils.events]: \u001b[0m eta: 1 day, 20:21:45  iter: 179  total_loss: 2.389  loss_cls: 1.045  loss_box_reg: 0.7588  loss_mask: 0.5399  loss_rpn_cls: 0.02739  loss_rpn_loc: 0.02361    time: 6.4644  last_time: 6.0284  data_time: 0.0243  last_data_time: 0.1372   lr: 0.00017982  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:29:18 d2.utils.events]: \u001b[0m eta: 1 day, 20:10:49  iter: 199  total_loss: 2.449  loss_cls: 1.051  loss_box_reg: 0.7781  loss_mask: 0.508  loss_rpn_cls: 0.03931  loss_rpn_loc: 0.02319    time: 6.4379  last_time: 4.1442  data_time: 0.0166  last_data_time: 0.0082   lr: 0.0001998  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:31:26 d2.utils.events]: \u001b[0m eta: 1 day, 20:00:11  iter: 219  total_loss: 2.18  loss_cls: 0.8498  loss_box_reg: 0.7687  loss_mask: 0.4609  loss_rpn_cls: 0.03347  loss_rpn_loc: 0.02705    time: 6.4346  last_time: 6.6805  data_time: 0.0286  last_data_time: 0.0881   lr: 0.00021978  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:33:23 d2.utils.events]: \u001b[0m eta: 1 day, 19:43:57  iter: 239  total_loss: 2.124  loss_cls: 0.808  loss_box_reg: 0.7151  loss_mask: 0.444  loss_rpn_cls: 0.02245  loss_rpn_loc: 0.02703    time: 6.3821  last_time: 6.0987  data_time: 0.0175  last_data_time: 0.0107   lr: 0.00023976  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:35:23 d2.utils.events]: \u001b[0m eta: 1 day, 19:32:34  iter: 259  total_loss: 2.114  loss_cls: 0.7535  loss_box_reg: 0.7557  loss_mask: 0.3954  loss_rpn_cls: 0.03126  loss_rpn_loc: 0.03092    time: 6.3551  last_time: 5.4891  data_time: 0.0288  last_data_time: 0.0050   lr: 0.00025974  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:37:32 d2.utils.events]: \u001b[0m eta: 1 day, 19:44:02  iter: 279  total_loss: 2.045  loss_cls: 0.9036  loss_box_reg: 0.7367  loss_mask: 0.3902  loss_rpn_cls: 0.02199  loss_rpn_loc: 0.01848    time: 6.3607  last_time: 6.4347  data_time: 0.0245  last_data_time: 0.0046   lr: 0.00027972  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:39:35 d2.utils.events]: \u001b[0m eta: 1 day, 19:37:35  iter: 299  total_loss: 1.892  loss_cls: 0.7468  loss_box_reg: 0.7035  loss_mask: 0.3447  loss_rpn_cls: 0.0267  loss_rpn_loc: 0.02604    time: 6.3479  last_time: 6.3309  data_time: 0.0343  last_data_time: 0.0071   lr: 0.0002997  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:41:35 d2.utils.events]: \u001b[0m eta: 1 day, 19:24:17  iter: 319  total_loss: 1.838  loss_cls: 0.7675  loss_box_reg: 0.6893  loss_mask: 0.3361  loss_rpn_cls: 0.02893  loss_rpn_loc: 0.02316    time: 6.3243  last_time: 4.9737  data_time: 0.0262  last_data_time: 0.0059   lr: 0.00031968  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:43:36 d2.utils.events]: \u001b[0m eta: 1 day, 19:13:36  iter: 339  total_loss: 1.932  loss_cls: 0.7788  loss_box_reg: 0.6816  loss_mask: 0.3321  loss_rpn_cls: 0.0261  loss_rpn_loc: 0.0219    time: 6.3078  last_time: 5.5580  data_time: 0.0394  last_data_time: 0.0742   lr: 0.00033966  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:45:34 d2.utils.events]: \u001b[0m eta: 1 day, 18:49:22  iter: 359  total_loss: 1.633  loss_cls: 0.6454  loss_box_reg: 0.6594  loss_mask: 0.2954  loss_rpn_cls: 0.01799  loss_rpn_loc: 0.01935    time: 6.2850  last_time: 5.9156  data_time: 0.0230  last_data_time: 0.0069   lr: 0.00035964  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:47:32 d2.utils.events]: \u001b[0m eta: 1 day, 18:45:10  iter: 379  total_loss: 1.784  loss_cls: 0.793  loss_box_reg: 0.6202  loss_mask: 0.2851  loss_rpn_cls: 0.02554  loss_rpn_loc: 0.02138    time: 6.2641  last_time: 5.5872  data_time: 0.0356  last_data_time: 0.0071   lr: 0.00037962  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:49:37 d2.utils.events]: \u001b[0m eta: 1 day, 18:43:05  iter: 399  total_loss: 1.545  loss_cls: 0.6795  loss_box_reg: 0.5732  loss_mask: 0.2555  loss_rpn_cls: 0.01995  loss_rpn_loc: 0.01704    time: 6.2645  last_time: 6.5517  data_time: 0.0202  last_data_time: 0.0081   lr: 0.0003996  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:51:47 d2.utils.events]: \u001b[0m eta: 1 day, 18:42:33  iter: 419  total_loss: 1.555  loss_cls: 0.6615  loss_box_reg: 0.5631  loss_mask: 0.2532  loss_rpn_cls: 0.02288  loss_rpn_loc: 0.02325    time: 6.2755  last_time: 6.4550  data_time: 0.0195  last_data_time: 0.0062   lr: 0.00041958  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:53:54 d2.utils.events]: \u001b[0m eta: 1 day, 18:40:28  iter: 439  total_loss: 1.56  loss_cls: 0.6695  loss_box_reg: 0.5532  loss_mask: 0.2595  loss_rpn_cls: 0.02559  loss_rpn_loc: 0.028    time: 6.2792  last_time: 7.9147  data_time: 0.0322  last_data_time: 0.0055   lr: 0.00043956  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:55:53 d2.utils.events]: \u001b[0m eta: 1 day, 18:31:57  iter: 459  total_loss: 1.557  loss_cls: 0.6873  loss_box_reg: 0.5674  loss_mask: 0.2525  loss_rpn_cls: 0.03026  loss_rpn_loc: 0.0251    time: 6.2637  last_time: 5.5013  data_time: 0.0375  last_data_time: 0.0039   lr: 0.00045954  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:57:51 d2.utils.events]: \u001b[0m eta: 1 day, 18:21:09  iter: 479  total_loss: 1.386  loss_cls: 0.538  loss_box_reg: 0.5103  loss_mask: 0.253  loss_rpn_cls: 0.019  loss_rpn_loc: 0.02534    time: 6.2489  last_time: 7.4743  data_time: 0.0282  last_data_time: 0.0068   lr: 0.00047952  max_mem: 8233M\n",
            "\u001b[32m[08/09 00:59:57 d2.utils.events]: \u001b[0m eta: 1 day, 18:26:23  iter: 499  total_loss: 1.449  loss_cls: 0.5821  loss_box_reg: 0.5018  loss_mask: 0.2309  loss_rpn_cls: 0.02294  loss_rpn_loc: 0.02046    time: 6.2495  last_time: 5.5824  data_time: 0.0166  last_data_time: 0.0056   lr: 0.0004995  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:01:57 d2.utils.events]: \u001b[0m eta: 1 day, 18:20:18  iter: 519  total_loss: 1.401  loss_cls: 0.6336  loss_box_reg: 0.527  loss_mask: 0.2343  loss_rpn_cls: 0.01745  loss_rpn_loc: 0.01982    time: 6.2402  last_time: 6.8429  data_time: 0.0227  last_data_time: 0.0078   lr: 0.00051948  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:03:56 d2.utils.events]: \u001b[0m eta: 1 day, 18:04:18  iter: 539  total_loss: 1.273  loss_cls: 0.5784  loss_box_reg: 0.4951  loss_mask: 0.2229  loss_rpn_cls: 0.02579  loss_rpn_loc: 0.01894    time: 6.2288  last_time: 5.6396  data_time: 0.0295  last_data_time: 0.0059   lr: 0.00053946  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:05:55 d2.utils.events]: \u001b[0m eta: 1 day, 18:01:19  iter: 559  total_loss: 1.463  loss_cls: 0.5978  loss_box_reg: 0.5178  loss_mask: 0.2502  loss_rpn_cls: 0.01966  loss_rpn_loc: 0.02052    time: 6.2184  last_time: 6.3221  data_time: 0.0253  last_data_time: 0.0076   lr: 0.00055944  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:07:53 d2.utils.events]: \u001b[0m eta: 1 day, 17:51:34  iter: 579  total_loss: 1.218  loss_cls: 0.5294  loss_box_reg: 0.4764  loss_mask: 0.2443  loss_rpn_cls: 0.02443  loss_rpn_loc: 0.03039    time: 6.2073  last_time: 5.5102  data_time: 0.0268  last_data_time: 0.0070   lr: 0.00057942  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:09:59 d2.utils.events]: \u001b[0m eta: 1 day, 17:49:31  iter: 599  total_loss: 1.364  loss_cls: 0.5728  loss_box_reg: 0.5098  loss_mask: 0.2308  loss_rpn_cls: 0.01669  loss_rpn_loc: 0.01871    time: 6.2099  last_time: 8.0832  data_time: 0.0202  last_data_time: 0.0049   lr: 0.0005994  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:12:04 d2.utils.events]: \u001b[0m eta: 1 day, 17:47:28  iter: 619  total_loss: 1.231  loss_cls: 0.5193  loss_box_reg: 0.4652  loss_mask: 0.2092  loss_rpn_cls: 0.01541  loss_rpn_loc: 0.01879    time: 6.2121  last_time: 7.1140  data_time: 0.0202  last_data_time: 0.0051   lr: 0.00061938  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:14:02 d2.utils.events]: \u001b[0m eta: 1 day, 17:44:43  iter: 639  total_loss: 1.356  loss_cls: 0.5966  loss_box_reg: 0.4768  loss_mask: 0.2191  loss_rpn_cls: 0.01747  loss_rpn_loc: 0.02989    time: 6.2025  last_time: 6.5988  data_time: 0.0375  last_data_time: 0.0790   lr: 0.00063936  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:16:04 d2.utils.events]: \u001b[0m eta: 1 day, 17:39:10  iter: 659  total_loss: 1.359  loss_cls: 0.5686  loss_box_reg: 0.4498  loss_mask: 0.2192  loss_rpn_cls: 0.01548  loss_rpn_loc: 0.02139    time: 6.1984  last_time: 7.1193  data_time: 0.0255  last_data_time: 0.0070   lr: 0.00065934  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:18:06 d2.utils.events]: \u001b[0m eta: 1 day, 17:37:43  iter: 679  total_loss: 1.399  loss_cls: 0.5925  loss_box_reg: 0.469  loss_mask: 0.2202  loss_rpn_cls: 0.0267  loss_rpn_loc: 0.03087    time: 6.1954  last_time: 5.4005  data_time: 0.0186  last_data_time: 0.0087   lr: 0.00067932  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:19:59 d2.utils.events]: \u001b[0m eta: 1 day, 17:34:08  iter: 699  total_loss: 1.116  loss_cls: 0.4652  loss_box_reg: 0.4581  loss_mask: 0.1863  loss_rpn_cls: 0.01288  loss_rpn_loc: 0.02011    time: 6.1801  last_time: 4.5783  data_time: 0.0332  last_data_time: 0.0047   lr: 0.0006993  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:22:03 d2.utils.events]: \u001b[0m eta: 1 day, 17:33:37  iter: 719  total_loss: 1.221  loss_cls: 0.5369  loss_box_reg: 0.4564  loss_mask: 0.2157  loss_rpn_cls: 0.01397  loss_rpn_loc: 0.0211    time: 6.1809  last_time: 4.7465  data_time: 0.0169  last_data_time: 0.0059   lr: 0.00071928  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:23:57 d2.utils.events]: \u001b[0m eta: 1 day, 17:26:02  iter: 739  total_loss: 1.168  loss_cls: 0.5085  loss_box_reg: 0.424  loss_mask: 0.2033  loss_rpn_cls: 0.01557  loss_rpn_loc: 0.02685    time: 6.1676  last_time: 4.4198  data_time: 0.0399  last_data_time: 0.0052   lr: 0.00073926  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:26:02 d2.utils.events]: \u001b[0m eta: 1 day, 17:27:59  iter: 759  total_loss: 1.137  loss_cls: 0.5146  loss_box_reg: 0.4213  loss_mask: 0.1835  loss_rpn_cls: 0.01262  loss_rpn_loc: 0.02305    time: 6.1703  last_time: 7.2993  data_time: 0.0352  last_data_time: 0.0051   lr: 0.00075924  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:27:56 d2.utils.events]: \u001b[0m eta: 1 day, 17:21:57  iter: 779  total_loss: 1.142  loss_cls: 0.5256  loss_box_reg: 0.4586  loss_mask: 0.2179  loss_rpn_cls: 0.009978  loss_rpn_loc: 0.02195    time: 6.1582  last_time: 4.8652  data_time: 0.0210  last_data_time: 0.0065   lr: 0.00077922  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:30:05 d2.utils.events]: \u001b[0m eta: 1 day, 17:23:01  iter: 799  total_loss: 1.146  loss_cls: 0.5482  loss_box_reg: 0.4063  loss_mask: 0.1795  loss_rpn_cls: 0.0121  loss_rpn_loc: 0.02514    time: 6.1647  last_time: 5.7817  data_time: 0.0265  last_data_time: 0.0068   lr: 0.0007992  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:32:14 d2.utils.events]: \u001b[0m eta: 1 day, 17:20:19  iter: 819  total_loss: 1.168  loss_cls: 0.5098  loss_box_reg: 0.4282  loss_mask: 0.1938  loss_rpn_cls: 0.01247  loss_rpn_loc: 0.01528    time: 6.1714  last_time: 8.3402  data_time: 0.0189  last_data_time: 0.0748   lr: 0.00081918  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:34:10 d2.utils.events]: \u001b[0m eta: 1 day, 17:17:26  iter: 839  total_loss: 1.148  loss_cls: 0.4613  loss_box_reg: 0.394  loss_mask: 0.2105  loss_rpn_cls: 0.01004  loss_rpn_loc: 0.01414    time: 6.1625  last_time: 6.2998  data_time: 0.0197  last_data_time: 0.0053   lr: 0.00083916  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:36:07 d2.utils.events]: \u001b[0m eta: 1 day, 17:08:08  iter: 859  total_loss: 1.07  loss_cls: 0.4783  loss_box_reg: 0.3829  loss_mask: 0.1854  loss_rpn_cls: 0.008276  loss_rpn_loc: 0.01665    time: 6.1554  last_time: 6.0883  data_time: 0.0371  last_data_time: 0.0074   lr: 0.00085914  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:38:08 d2.utils.events]: \u001b[0m eta: 1 day, 17:06:06  iter: 879  total_loss: 1.267  loss_cls: 0.5524  loss_box_reg: 0.442  loss_mask: 0.2053  loss_rpn_cls: 0.01776  loss_rpn_loc: 0.02491    time: 6.1530  last_time: 5.0875  data_time: 0.0245  last_data_time: 0.0080   lr: 0.00087912  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:40:12 d2.utils.events]: \u001b[0m eta: 1 day, 17:03:31  iter: 899  total_loss: 1.207  loss_cls: 0.5054  loss_box_reg: 0.4087  loss_mask: 0.1967  loss_rpn_cls: 0.0176  loss_rpn_loc: 0.02386    time: 6.1544  last_time: 6.2624  data_time: 0.0250  last_data_time: 0.0051   lr: 0.0008991  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:42:19 d2.utils.events]: \u001b[0m eta: 1 day, 17:02:56  iter: 919  total_loss: 1.165  loss_cls: 0.4871  loss_box_reg: 0.4113  loss_mask: 0.1856  loss_rpn_cls: 0.01215  loss_rpn_loc: 0.01644    time: 6.1580  last_time: 6.5804  data_time: 0.0304  last_data_time: 0.0063   lr: 0.00091908  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:44:21 d2.utils.events]: \u001b[0m eta: 1 day, 17:00:09  iter: 939  total_loss: 1.033  loss_cls: 0.4579  loss_box_reg: 0.4048  loss_mask: 0.1871  loss_rpn_cls: 0.01192  loss_rpn_loc: 0.01669    time: 6.1570  last_time: 5.0594  data_time: 0.0125  last_data_time: 0.0048   lr: 0.00093906  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:46:20 d2.utils.events]: \u001b[0m eta: 1 day, 16:57:55  iter: 959  total_loss: 1.212  loss_cls: 0.5  loss_box_reg: 0.4448  loss_mask: 0.2221  loss_rpn_cls: 0.01456  loss_rpn_loc: 0.0225    time: 6.1531  last_time: 5.9692  data_time: 0.0273  last_data_time: 0.0080   lr: 0.00095904  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:48:20 d2.utils.events]: \u001b[0m eta: 1 day, 16:55:52  iter: 979  total_loss: 1.181  loss_cls: 0.4939  loss_box_reg: 0.4239  loss_mask: 0.1978  loss_rpn_cls: 0.0111  loss_rpn_loc: 0.02072    time: 6.1494  last_time: 5.9692  data_time: 0.0233  last_data_time: 0.0081   lr: 0.00097902  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:50:22 d2.utils.events]: \u001b[0m eta: 1 day, 16:52:37  iter: 999  total_loss: 1.127  loss_cls: 0.4715  loss_box_reg: 0.3664  loss_mask: 0.1743  loss_rpn_cls: 0.009757  loss_rpn_loc: 0.01657    time: 6.1477  last_time: 7.5309  data_time: 0.0258  last_data_time: 0.1346   lr: 0.000999  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:52:27 d2.utils.events]: \u001b[0m eta: 1 day, 16:47:52  iter: 1019  total_loss: 1.206  loss_cls: 0.5619  loss_box_reg: 0.4214  loss_mask: 0.1926  loss_rpn_cls: 0.01289  loss_rpn_loc: 0.02354    time: 6.1498  last_time: 4.6287  data_time: 0.0225  last_data_time: 0.0056   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:54:26 d2.utils.events]: \u001b[0m eta: 1 day, 16:42:34  iter: 1039  total_loss: 1.111  loss_cls: 0.4826  loss_box_reg: 0.4045  loss_mask: 0.1864  loss_rpn_cls: 0.01541  loss_rpn_loc: 0.02659    time: 6.1465  last_time: 6.8216  data_time: 0.0329  last_data_time: 0.2257   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:56:30 d2.utils.events]: \u001b[0m eta: 1 day, 16:31:09  iter: 1059  total_loss: 1.008  loss_cls: 0.4373  loss_box_reg: 0.3846  loss_mask: 0.1819  loss_rpn_cls: 0.005602  loss_rpn_loc: 0.01411    time: 6.1471  last_time: 7.0920  data_time: 0.0309  last_data_time: 0.0061   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 01:58:31 d2.utils.events]: \u001b[0m eta: 1 day, 16:28:11  iter: 1079  total_loss: 1.167  loss_cls: 0.4589  loss_box_reg: 0.4041  loss_mask: 0.206  loss_rpn_cls: 0.01478  loss_rpn_loc: 0.02627    time: 6.1452  last_time: 4.9672  data_time: 0.0253  last_data_time: 0.0050   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:00:27 d2.utils.events]: \u001b[0m eta: 1 day, 16:23:02  iter: 1099  total_loss: 1.016  loss_cls: 0.4539  loss_box_reg: 0.3759  loss_mask: 0.1624  loss_rpn_cls: 0.01395  loss_rpn_loc: 0.02493    time: 6.1386  last_time: 6.3710  data_time: 0.0309  last_data_time: 0.0059   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:02:32 d2.utils.events]: \u001b[0m eta: 1 day, 16:20:10  iter: 1119  total_loss: 0.9935  loss_cls: 0.4193  loss_box_reg: 0.3644  loss_mask: 0.1759  loss_rpn_cls: 0.009655  loss_rpn_loc: 0.02333    time: 6.1408  last_time: 6.9480  data_time: 0.0187  last_data_time: 0.0062   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:04:38 d2.utils.events]: \u001b[0m eta: 1 day, 16:19:24  iter: 1139  total_loss: 1.029  loss_cls: 0.4385  loss_box_reg: 0.3681  loss_mask: 0.1648  loss_rpn_cls: 0.007263  loss_rpn_loc: 0.01618    time: 6.1436  last_time: 6.1278  data_time: 0.0268  last_data_time: 0.0660   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:06:36 d2.utils.events]: \u001b[0m eta: 1 day, 16:13:41  iter: 1159  total_loss: 1.071  loss_cls: 0.4303  loss_box_reg: 0.3996  loss_mask: 0.1769  loss_rpn_cls: 0.01032  loss_rpn_loc: 0.01953    time: 6.1394  last_time: 6.2428  data_time: 0.0298  last_data_time: 0.0066   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:08:43 d2.utils.events]: \u001b[0m eta: 1 day, 16:12:58  iter: 1179  total_loss: 0.9571  loss_cls: 0.4364  loss_box_reg: 0.3442  loss_mask: 0.1824  loss_rpn_cls: 0.007897  loss_rpn_loc: 0.01749    time: 6.1426  last_time: 7.4285  data_time: 0.0337  last_data_time: 0.0075   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:10:39 d2.utils.events]: \u001b[0m eta: 1 day, 16:07:18  iter: 1199  total_loss: 1.004  loss_cls: 0.3802  loss_box_reg: 0.3555  loss_mask: 0.1926  loss_rpn_cls: 0.0119  loss_rpn_loc: 0.02255    time: 6.1377  last_time: 7.5119  data_time: 0.0231  last_data_time: 0.0054   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:12:33 d2.utils.events]: \u001b[0m eta: 1 day, 16:03:06  iter: 1219  total_loss: 0.9454  loss_cls: 0.4464  loss_box_reg: 0.3421  loss_mask: 0.165  loss_rpn_cls: 0.008786  loss_rpn_loc: 0.01398    time: 6.1297  last_time: 6.4736  data_time: 0.0242  last_data_time: 0.0100   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:14:42 d2.utils.events]: \u001b[0m eta: 1 day, 16:02:58  iter: 1239  total_loss: 1.016  loss_cls: 0.4298  loss_box_reg: 0.3785  loss_mask: 0.1844  loss_rpn_cls: 0.0104  loss_rpn_loc: 0.02104    time: 6.1354  last_time: 7.0640  data_time: 0.0344  last_data_time: 0.0774   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:16:47 d2.utils.events]: \u001b[0m eta: 1 day, 16:00:32  iter: 1259  total_loss: 1.094  loss_cls: 0.4751  loss_box_reg: 0.3789  loss_mask: 0.1784  loss_rpn_cls: 0.007911  loss_rpn_loc: 0.02013    time: 6.1370  last_time: 5.6467  data_time: 0.0264  last_data_time: 0.0771   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:18:52 d2.utils.events]: \u001b[0m eta: 1 day, 15:57:31  iter: 1279  total_loss: 0.9219  loss_cls: 0.395  loss_box_reg: 0.345  loss_mask: 0.1579  loss_rpn_cls: 0.007477  loss_rpn_loc: 0.01803    time: 6.1388  last_time: 5.7268  data_time: 0.0260  last_data_time: 0.0424   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:21:00 d2.utils.events]: \u001b[0m eta: 1 day, 15:56:30  iter: 1299  total_loss: 1.073  loss_cls: 0.4325  loss_box_reg: 0.3751  loss_mask: 0.1788  loss_rpn_cls: 0.0105  loss_rpn_loc: 0.01963    time: 6.1425  last_time: 5.9788  data_time: 0.0326  last_data_time: 0.0040   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:23:01 d2.utils.events]: \u001b[0m eta: 1 day, 15:54:48  iter: 1319  total_loss: 1.014  loss_cls: 0.3961  loss_box_reg: 0.3574  loss_mask: 0.1776  loss_rpn_cls: 0.01056  loss_rpn_loc: 0.02222    time: 6.1411  last_time: 4.2162  data_time: 0.0340  last_data_time: 0.0082   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:25:06 d2.utils.events]: \u001b[0m eta: 1 day, 15:55:27  iter: 1339  total_loss: 1.044  loss_cls: 0.4399  loss_box_reg: 0.376  loss_mask: 0.1791  loss_rpn_cls: 0.009415  loss_rpn_loc: 0.01394    time: 6.1427  last_time: 6.1513  data_time: 0.0133  last_data_time: 0.0042   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:27:15 d2.utils.events]: \u001b[0m eta: 1 day, 15:56:31  iter: 1359  total_loss: 1.032  loss_cls: 0.4608  loss_box_reg: 0.3513  loss_mask: 0.1652  loss_rpn_cls: 0.008077  loss_rpn_loc: 0.02163    time: 6.1472  last_time: 7.0628  data_time: 0.0161  last_data_time: 0.0062   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:29:21 d2.utils.events]: \u001b[0m eta: 1 day, 15:54:37  iter: 1379  total_loss: 0.8711  loss_cls: 0.3681  loss_box_reg: 0.3404  loss_mask: 0.1509  loss_rpn_cls: 0.009738  loss_rpn_loc: 0.01745    time: 6.1495  last_time: 6.5584  data_time: 0.0276  last_data_time: 0.0058   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:31:27 d2.utils.events]: \u001b[0m eta: 1 day, 15:52:36  iter: 1399  total_loss: 0.8893  loss_cls: 0.3728  loss_box_reg: 0.314  loss_mask: 0.1753  loss_rpn_cls: 0.01132  loss_rpn_loc: 0.0232    time: 6.1518  last_time: 6.7136  data_time: 0.0198  last_data_time: 0.0059   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:33:32 d2.utils.events]: \u001b[0m eta: 1 day, 15:50:26  iter: 1419  total_loss: 1.058  loss_cls: 0.4438  loss_box_reg: 0.3915  loss_mask: 0.1878  loss_rpn_cls: 0.006568  loss_rpn_loc: 0.02333    time: 6.1535  last_time: 5.3266  data_time: 0.0210  last_data_time: 0.0080   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:35:35 d2.utils.events]: \u001b[0m eta: 1 day, 15:48:24  iter: 1439  total_loss: 0.8774  loss_cls: 0.3993  loss_box_reg: 0.3516  loss_mask: 0.1717  loss_rpn_cls: 0.009073  loss_rpn_loc: 0.01447    time: 6.1528  last_time: 6.8407  data_time: 0.0279  last_data_time: 0.0042   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:37:36 d2.utils.events]: \u001b[0m eta: 1 day, 15:46:23  iter: 1459  total_loss: 0.9237  loss_cls: 0.4063  loss_box_reg: 0.3345  loss_mask: 0.1537  loss_rpn_cls: 0.01285  loss_rpn_loc: 0.0212    time: 6.1517  last_time: 6.8713  data_time: 0.0260  last_data_time: 0.1533   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:39:38 d2.utils.events]: \u001b[0m eta: 1 day, 15:44:29  iter: 1479  total_loss: 0.8992  loss_cls: 0.3946  loss_box_reg: 0.3469  loss_mask: 0.1593  loss_rpn_cls: 0.006681  loss_rpn_loc: 0.01349    time: 6.1507  last_time: 4.8754  data_time: 0.0215  last_data_time: 0.0043   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:41:42 d2.utils.events]: \u001b[0m eta: 1 day, 15:41:28  iter: 1499  total_loss: 0.968  loss_cls: 0.4032  loss_box_reg: 0.3212  loss_mask: 0.1694  loss_rpn_cls: 0.009981  loss_rpn_loc: 0.0174    time: 6.1508  last_time: 7.9882  data_time: 0.0166  last_data_time: 0.0064   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:43:46 d2.utils.events]: \u001b[0m eta: 1 day, 15:38:20  iter: 1519  total_loss: 0.9105  loss_cls: 0.3602  loss_box_reg: 0.351  loss_mask: 0.1614  loss_rpn_cls: 0.006012  loss_rpn_loc: 0.02066    time: 6.1516  last_time: 5.6247  data_time: 0.0239  last_data_time: 0.0895   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:45:52 d2.utils.events]: \u001b[0m eta: 1 day, 15:40:59  iter: 1539  total_loss: 0.906  loss_cls: 0.3641  loss_box_reg: 0.3193  loss_mask: 0.1667  loss_rpn_cls: 0.006458  loss_rpn_loc: 0.0244    time: 6.1534  last_time: 5.2982  data_time: 0.0245  last_data_time: 0.0063   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:47:50 d2.utils.events]: \u001b[0m eta: 1 day, 15:36:58  iter: 1559  total_loss: 0.9139  loss_cls: 0.3554  loss_box_reg: 0.3406  loss_mask: 0.1705  loss_rpn_cls: 0.005177  loss_rpn_loc: 0.01694    time: 6.1502  last_time: 5.9434  data_time: 0.0311  last_data_time: 0.0907   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:49:50 d2.utils.events]: \u001b[0m eta: 1 day, 15:35:20  iter: 1579  total_loss: 0.852  loss_cls: 0.3973  loss_box_reg: 0.3272  loss_mask: 0.1725  loss_rpn_cls: 0.007766  loss_rpn_loc: 0.01715    time: 6.1486  last_time: 5.8620  data_time: 0.0267  last_data_time: 0.0040   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:51:50 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:55  iter: 1599  total_loss: 0.8196  loss_cls: 0.3747  loss_box_reg: 0.3083  loss_mask: 0.1648  loss_rpn_cls: 0.005092  loss_rpn_loc: 0.01467    time: 6.1468  last_time: 6.8163  data_time: 0.0172  last_data_time: 0.0066   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:53:59 d2.utils.events]: \u001b[0m eta: 1 day, 15:31:17  iter: 1619  total_loss: 0.8359  loss_cls: 0.3528  loss_box_reg: 0.3088  loss_mask: 0.1538  loss_rpn_cls: 0.005416  loss_rpn_loc: 0.01765    time: 6.1506  last_time: 6.4178  data_time: 0.0229  last_data_time: 0.0652   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:56:09 d2.utils.events]: \u001b[0m eta: 1 day, 15:35:12  iter: 1639  total_loss: 0.915  loss_cls: 0.3716  loss_box_reg: 0.3447  loss_mask: 0.1785  loss_rpn_cls: 0.01016  loss_rpn_loc: 0.02831    time: 6.1544  last_time: 7.7061  data_time: 0.0213  last_data_time: 0.0066   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 02:58:04 d2.utils.events]: \u001b[0m eta: 1 day, 15:29:43  iter: 1659  total_loss: 0.9573  loss_cls: 0.4249  loss_box_reg: 0.3187  loss_mask: 0.1777  loss_rpn_cls: 0.009542  loss_rpn_loc: 0.01757    time: 6.1499  last_time: 4.3621  data_time: 0.0338  last_data_time: 0.0053   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:00:10 d2.utils.events]: \u001b[0m eta: 1 day, 15:27:41  iter: 1679  total_loss: 0.8289  loss_cls: 0.3371  loss_box_reg: 0.3012  loss_mask: 0.1492  loss_rpn_cls: 0.008178  loss_rpn_loc: 0.01403    time: 6.1513  last_time: 5.4216  data_time: 0.0262  last_data_time: 0.0500   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:02:16 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:01  iter: 1699  total_loss: 0.8959  loss_cls: 0.393  loss_box_reg: 0.3152  loss_mask: 0.1531  loss_rpn_cls: 0.005058  loss_rpn_loc: 0.01682    time: 6.1533  last_time: 7.1380  data_time: 0.0112  last_data_time: 0.0061   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:04:25 d2.utils.events]: \u001b[0m eta: 1 day, 15:24:08  iter: 1719  total_loss: 0.8311  loss_cls: 0.3279  loss_box_reg: 0.2938  loss_mask: 0.1562  loss_rpn_cls: 0.007605  loss_rpn_loc: 0.01749    time: 6.1566  last_time: 5.1987  data_time: 0.0423  last_data_time: 0.0056   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:06:25 d2.utils.events]: \u001b[0m eta: 1 day, 15:25:02  iter: 1739  total_loss: 0.8509  loss_cls: 0.3587  loss_box_reg: 0.3096  loss_mask: 0.1592  loss_rpn_cls: 0.005251  loss_rpn_loc: 0.01865    time: 6.1546  last_time: 5.8170  data_time: 0.0228  last_data_time: 0.0066   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:08:26 d2.utils.events]: \u001b[0m eta: 1 day, 15:18:39  iter: 1759  total_loss: 0.7847  loss_cls: 0.3251  loss_box_reg: 0.3299  loss_mask: 0.1572  loss_rpn_cls: 0.0066  loss_rpn_loc: 0.01802    time: 6.1536  last_time: 5.1807  data_time: 0.0173  last_data_time: 0.0058   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:10:35 d2.utils.events]: \u001b[0m eta: 1 day, 15:24:54  iter: 1779  total_loss: 0.8918  loss_cls: 0.3702  loss_box_reg: 0.3356  loss_mask: 0.1553  loss_rpn_cls: 0.0075  loss_rpn_loc: 0.01865    time: 6.1568  last_time: 7.1654  data_time: 0.0200  last_data_time: 0.0086   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:12:42 d2.utils.events]: \u001b[0m eta: 1 day, 15:21:50  iter: 1799  total_loss: 0.8957  loss_cls: 0.3724  loss_box_reg: 0.3297  loss_mask: 0.1602  loss_rpn_cls: 0.005384  loss_rpn_loc: 0.0173    time: 6.1588  last_time: 6.9044  data_time: 0.0235  last_data_time: 0.0048   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:14:50 d2.utils.events]: \u001b[0m eta: 1 day, 15:19:59  iter: 1819  total_loss: 0.8379  loss_cls: 0.3107  loss_box_reg: 0.3052  loss_mask: 0.1569  loss_rpn_cls: 0.006832  loss_rpn_loc: 0.01692    time: 6.1616  last_time: 5.6696  data_time: 0.0316  last_data_time: 0.0058   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:16:49 d2.utils.events]: \u001b[0m eta: 1 day, 15:17:57  iter: 1839  total_loss: 0.7653  loss_cls: 0.2953  loss_box_reg: 0.2866  loss_mask: 0.1319  loss_rpn_cls: 0.005253  loss_rpn_loc: 0.01304    time: 6.1593  last_time: 6.9517  data_time: 0.0218  last_data_time: 0.0057   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:18:47 d2.utils.events]: \u001b[0m eta: 1 day, 15:18:19  iter: 1859  total_loss: 0.8863  loss_cls: 0.3343  loss_box_reg: 0.3182  loss_mask: 0.1667  loss_rpn_cls: 0.006909  loss_rpn_loc: 0.02031    time: 6.1564  last_time: 5.5427  data_time: 0.0320  last_data_time: 0.0063   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:20:53 d2.utils.events]: \u001b[0m eta: 1 day, 15:15:26  iter: 1879  total_loss: 0.8232  loss_cls: 0.2908  loss_box_reg: 0.297  loss_mask: 0.1618  loss_rpn_cls: 0.004239  loss_rpn_loc: 0.0153    time: 6.1580  last_time: 6.9508  data_time: 0.0269  last_data_time: 0.0070   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:22:51 d2.utils.events]: \u001b[0m eta: 1 day, 15:11:51  iter: 1899  total_loss: 0.8659  loss_cls: 0.3512  loss_box_reg: 0.3396  loss_mask: 0.1547  loss_rpn_cls: 0.01004  loss_rpn_loc: 0.01801    time: 6.1552  last_time: 7.9065  data_time: 0.0175  last_data_time: 0.0056   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:24:59 d2.utils.events]: \u001b[0m eta: 1 day, 15:09:49  iter: 1919  total_loss: 0.9137  loss_cls: 0.3345  loss_box_reg: 0.3362  loss_mask: 0.1784  loss_rpn_cls: 0.008036  loss_rpn_loc: 0.02038    time: 6.1577  last_time: 7.9983  data_time: 0.0151  last_data_time: 0.0063   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:26:59 d2.utils.events]: \u001b[0m eta: 1 day, 15:08:36  iter: 1939  total_loss: 0.8129  loss_cls: 0.3354  loss_box_reg: 0.2975  loss_mask: 0.1433  loss_rpn_cls: 0.006485  loss_rpn_loc: 0.01441    time: 6.1562  last_time: 3.7813  data_time: 0.0450  last_data_time: 0.0861   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:29:05 d2.utils.events]: \u001b[0m eta: 1 day, 15:06:34  iter: 1959  total_loss: 0.8839  loss_cls: 0.3127  loss_box_reg: 0.3315  loss_mask: 0.1752  loss_rpn_cls: 0.008122  loss_rpn_loc: 0.01519    time: 6.1578  last_time: 5.5774  data_time: 0.0158  last_data_time: 0.0067   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:31:03 d2.utils.events]: \u001b[0m eta: 1 day, 15:03:42  iter: 1979  total_loss: 0.8529  loss_cls: 0.3317  loss_box_reg: 0.3178  loss_mask: 0.1528  loss_rpn_cls: 0.007451  loss_rpn_loc: 0.01916    time: 6.1552  last_time: 5.1761  data_time: 0.0201  last_data_time: 0.0060   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:33:10 d2.utils.events]: \u001b[0m eta: 1 day, 15:05:44  iter: 1999  total_loss: 0.8941  loss_cls: 0.3691  loss_box_reg: 0.3199  loss_mask: 0.1658  loss_rpn_cls: 0.005552  loss_rpn_loc: 0.01724    time: 6.1565  last_time: 6.3917  data_time: 0.0373  last_data_time: 0.0053   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:35:17 d2.utils.events]: \u001b[0m eta: 1 day, 15:07:22  iter: 2019  total_loss: 0.8025  loss_cls: 0.3974  loss_box_reg: 0.2881  loss_mask: 0.146  loss_rpn_cls: 0.004377  loss_rpn_loc: 0.01261    time: 6.1588  last_time: 5.7824  data_time: 0.0238  last_data_time: 0.0058   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:37:24 d2.utils.events]: \u001b[0m eta: 1 day, 15:09:42  iter: 2039  total_loss: 0.8908  loss_cls: 0.4002  loss_box_reg: 0.2946  loss_mask: 0.1347  loss_rpn_cls: 0.00561  loss_rpn_loc: 0.01692    time: 6.1602  last_time: 5.2396  data_time: 0.0450  last_data_time: 0.1508   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:39:20 d2.utils.events]: \u001b[0m eta: 1 day, 15:04:14  iter: 2059  total_loss: 0.8213  loss_cls: 0.311  loss_box_reg: 0.3193  loss_mask: 0.1511  loss_rpn_cls: 0.008153  loss_rpn_loc: 0.0173    time: 6.1568  last_time: 7.1725  data_time: 0.0192  last_data_time: 0.0073   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:41:21 d2.utils.events]: \u001b[0m eta: 1 day, 15:02:11  iter: 2079  total_loss: 0.811  loss_cls: 0.3097  loss_box_reg: 0.2882  loss_mask: 0.1608  loss_rpn_cls: 0.006456  loss_rpn_loc: 0.01654    time: 6.1558  last_time: 7.1419  data_time: 0.0214  last_data_time: 0.0041   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:43:27 d2.utils.events]: \u001b[0m eta: 1 day, 15:04:20  iter: 2099  total_loss: 0.7865  loss_cls: 0.3161  loss_box_reg: 0.2893  loss_mask: 0.1409  loss_rpn_cls: 0.00666  loss_rpn_loc: 0.01761    time: 6.1569  last_time: 6.4472  data_time: 0.0293  last_data_time: 0.0058   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:45:24 d2.utils.events]: \u001b[0m eta: 1 day, 14:59:50  iter: 2119  total_loss: 0.813  loss_cls: 0.3185  loss_box_reg: 0.307  loss_mask: 0.1505  loss_rpn_cls: 0.005593  loss_rpn_loc: 0.01499    time: 6.1542  last_time: 6.1532  data_time: 0.0218  last_data_time: 0.0067   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:47:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:53:43  iter: 2139  total_loss: 0.7795  loss_cls: 0.3199  loss_box_reg: 0.3054  loss_mask: 0.1579  loss_rpn_cls: 0.005467  loss_rpn_loc: 0.01492    time: 6.1539  last_time: 7.1575  data_time: 0.0302  last_data_time: 0.0040   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:49:29 d2.utils.events]: \u001b[0m eta: 1 day, 14:55:45  iter: 2159  total_loss: 0.8574  loss_cls: 0.3384  loss_box_reg: 0.3043  loss_mask: 0.1609  loss_rpn_cls: 0.007839  loss_rpn_loc: 0.02297    time: 6.1537  last_time: 5.3543  data_time: 0.0231  last_data_time: 0.0061   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:51:31 d2.utils.events]: \u001b[0m eta: 1 day, 14:49:38  iter: 2179  total_loss: 0.7514  loss_cls: 0.3162  loss_box_reg: 0.3012  loss_mask: 0.1383  loss_rpn_cls: 0.006357  loss_rpn_loc: 0.01527    time: 6.1531  last_time: 5.7642  data_time: 0.0230  last_data_time: 0.0064   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:53:29 d2.utils.events]: \u001b[0m eta: 1 day, 14:49:55  iter: 2199  total_loss: 0.8359  loss_cls: 0.3308  loss_box_reg: 0.2762  loss_mask: 0.167  loss_rpn_cls: 0.006569  loss_rpn_loc: 0.01341    time: 6.1508  last_time: 6.2938  data_time: 0.0342  last_data_time: 0.0906   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:55:33 d2.utils.events]: \u001b[0m eta: 1 day, 14:50:44  iter: 2219  total_loss: 0.7987  loss_cls: 0.3099  loss_box_reg: 0.2979  loss_mask: 0.139  loss_rpn_cls: 0.005141  loss_rpn_loc: 0.01818    time: 6.1513  last_time: 5.5876  data_time: 0.0146  last_data_time: 0.0047   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:57:40 d2.utils.events]: \u001b[0m eta: 1 day, 14:49:14  iter: 2239  total_loss: 0.8298  loss_cls: 0.3444  loss_box_reg: 0.2768  loss_mask: 0.137  loss_rpn_cls: 0.004801  loss_rpn_loc: 0.01913    time: 6.1528  last_time: 6.1671  data_time: 0.0456  last_data_time: 0.0062   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 03:59:45 d2.utils.events]: \u001b[0m eta: 1 day, 14:51:16  iter: 2259  total_loss: 0.8071  loss_cls: 0.3287  loss_box_reg: 0.2791  loss_mask: 0.1534  loss_rpn_cls: 0.007347  loss_rpn_loc: 0.01962    time: 6.1538  last_time: 5.9834  data_time: 0.0252  last_data_time: 0.0041   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:01:55 d2.utils.events]: \u001b[0m eta: 1 day, 14:49:13  iter: 2279  total_loss: 0.8765  loss_cls: 0.3333  loss_box_reg: 0.3257  loss_mask: 0.1714  loss_rpn_cls: 0.006338  loss_rpn_loc: 0.0234    time: 6.1570  last_time: 4.0895  data_time: 0.0143  last_data_time: 0.0055   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:03:56 d2.utils.events]: \u001b[0m eta: 1 day, 14:47:10  iter: 2299  total_loss: 0.8061  loss_cls: 0.3222  loss_box_reg: 0.2895  loss_mask: 0.1541  loss_rpn_cls: 0.003768  loss_rpn_loc: 0.01717    time: 6.1561  last_time: 5.9693  data_time: 0.0201  last_data_time: 0.0057   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:06:03 d2.utils.events]: \u001b[0m eta: 1 day, 14:45:24  iter: 2319  total_loss: 0.748  loss_cls: 0.256  loss_box_reg: 0.2846  loss_mask: 0.1453  loss_rpn_cls: 0.006028  loss_rpn_loc: 0.01321    time: 6.1576  last_time: 6.6606  data_time: 0.0165  last_data_time: 0.0057   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:08:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:43:18  iter: 2339  total_loss: 0.721  loss_cls: 0.2922  loss_box_reg: 0.2727  loss_mask: 0.1447  loss_rpn_cls: 0.003316  loss_rpn_loc: 0.01431    time: 6.1570  last_time: 7.3022  data_time: 0.0149  last_data_time: 0.0058   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:10:00 d2.utils.events]: \u001b[0m eta: 1 day, 14:36:57  iter: 2359  total_loss: 0.8191  loss_cls: 0.3488  loss_box_reg: 0.2922  loss_mask: 0.1526  loss_rpn_cls: 0.004041  loss_rpn_loc: 0.01831    time: 6.1538  last_time: 5.3749  data_time: 0.0354  last_data_time: 0.0054   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:12:00 d2.utils.events]: \u001b[0m eta: 1 day, 14:34:22  iter: 2379  total_loss: 0.7504  loss_cls: 0.2866  loss_box_reg: 0.2772  loss_mask: 0.1439  loss_rpn_cls: 0.003649  loss_rpn_loc: 0.01723    time: 6.1525  last_time: 5.4771  data_time: 0.0356  last_data_time: 0.0115   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:13:56 d2.utils.events]: \u001b[0m eta: 1 day, 14:29:29  iter: 2399  total_loss: 0.775  loss_cls: 0.2777  loss_box_reg: 0.2717  loss_mask: 0.1543  loss_rpn_cls: 0.006255  loss_rpn_loc: 0.01117    time: 6.1493  last_time: 4.9333  data_time: 0.0315  last_data_time: 0.0067   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:16:00 d2.utils.events]: \u001b[0m eta: 1 day, 14:21:39  iter: 2419  total_loss: 0.8272  loss_cls: 0.2903  loss_box_reg: 0.2917  loss_mask: 0.1435  loss_rpn_cls: 0.006993  loss_rpn_loc: 0.02005    time: 6.1499  last_time: 6.2705  data_time: 0.0382  last_data_time: 0.0038   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:18:01 d2.utils.events]: \u001b[0m eta: 1 day, 14:17:54  iter: 2439  total_loss: 0.8094  loss_cls: 0.3489  loss_box_reg: 0.2768  loss_mask: 0.1511  loss_rpn_cls: 0.008281  loss_rpn_loc: 0.02096    time: 6.1489  last_time: 4.2432  data_time: 0.0320  last_data_time: 0.0063   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:19:58 d2.utils.events]: \u001b[0m eta: 1 day, 14:15:52  iter: 2459  total_loss: 0.7264  loss_cls: 0.2974  loss_box_reg: 0.2622  loss_mask: 0.1392  loss_rpn_cls: 0.003368  loss_rpn_loc: 0.01234    time: 6.1465  last_time: 5.0484  data_time: 0.0353  last_data_time: 0.0573   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:21:53 d2.utils.events]: \u001b[0m eta: 1 day, 14:13:13  iter: 2479  total_loss: 0.7304  loss_cls: 0.2841  loss_box_reg: 0.2601  loss_mask: 0.1515  loss_rpn_cls: 0.003349  loss_rpn_loc: 0.01649    time: 6.1433  last_time: 6.4173  data_time: 0.0228  last_data_time: 0.0060   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:23:55 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:53  iter: 2499  total_loss: 0.7842  loss_cls: 0.3493  loss_box_reg: 0.3026  loss_mask: 0.1397  loss_rpn_cls: 0.003187  loss_rpn_loc: 0.01793    time: 6.1428  last_time: 4.5471  data_time: 0.0222  last_data_time: 0.0109   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:25:57 d2.utils.events]: \u001b[0m eta: 1 day, 14:09:45  iter: 2519  total_loss: 0.7692  loss_cls: 0.2554  loss_box_reg: 0.2858  loss_mask: 0.1535  loss_rpn_cls: 0.005441  loss_rpn_loc: 0.01649    time: 6.1423  last_time: 6.6638  data_time: 0.0153  last_data_time: 0.0061   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:27:59 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:36  iter: 2539  total_loss: 0.6897  loss_cls: 0.2903  loss_box_reg: 0.2405  loss_mask: 0.1274  loss_rpn_cls: 0.002961  loss_rpn_loc: 0.01335    time: 6.1420  last_time: 4.6813  data_time: 0.0294  last_data_time: 0.0056   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:29:58 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:46  iter: 2559  total_loss: 0.726  loss_cls: 0.3011  loss_box_reg: 0.2709  loss_mask: 0.141  loss_rpn_cls: 0.004812  loss_rpn_loc: 0.01387    time: 6.1404  last_time: 4.1122  data_time: 0.0216  last_data_time: 0.0066   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:31:56 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:06  iter: 2579  total_loss: 0.8429  loss_cls: 0.3331  loss_box_reg: 0.3058  loss_mask: 0.1607  loss_rpn_cls: 0.004095  loss_rpn_loc: 0.01966    time: 6.1386  last_time: 4.6276  data_time: 0.0274  last_data_time: 0.0066   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:34:04 d2.utils.events]: \u001b[0m eta: 1 day, 14:11:51  iter: 2599  total_loss: 0.7216  loss_cls: 0.2699  loss_box_reg: 0.2627  loss_mask: 0.1342  loss_rpn_cls: 0.003799  loss_rpn_loc: 0.01122    time: 6.1406  last_time: 5.7303  data_time: 0.0163  last_data_time: 0.0076   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:36:10 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:00  iter: 2619  total_loss: 0.8253  loss_cls: 0.3581  loss_box_reg: 0.2711  loss_mask: 0.1488  loss_rpn_cls: 0.005915  loss_rpn_loc: 0.028    time: 6.1417  last_time: 5.0516  data_time: 0.0276  last_data_time: 0.0052   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:38:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:15  iter: 2639  total_loss: 0.7347  loss_cls: 0.2673  loss_box_reg: 0.29  loss_mask: 0.1247  loss_rpn_cls: 0.003136  loss_rpn_loc: 0.01591    time: 6.1407  last_time: 6.4160  data_time: 0.0243  last_data_time: 0.0059   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:40:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:59:54  iter: 2659  total_loss: 0.7598  loss_cls: 0.33  loss_box_reg: 0.2852  loss_mask: 0.1539  loss_rpn_cls: 0.007036  loss_rpn_loc: 0.01751    time: 6.1397  last_time: 6.7780  data_time: 0.0180  last_data_time: 0.0078   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:42:16 d2.utils.events]: \u001b[0m eta: 1 day, 13:57:51  iter: 2679  total_loss: 0.7517  loss_cls: 0.2888  loss_box_reg: 0.283  loss_mask: 0.1384  loss_rpn_cls: 0.004972  loss_rpn_loc: 0.01714    time: 6.1408  last_time: 6.9124  data_time: 0.0388  last_data_time: 0.1366   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:44:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:53  iter: 2699  total_loss: 0.6738  loss_cls: 0.2462  loss_box_reg: 0.2764  loss_mask: 0.131  loss_rpn_cls: 0.002396  loss_rpn_loc: 0.01203    time: 6.1407  last_time: 5.2532  data_time: 0.0194  last_data_time: 0.0037   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:46:16 d2.utils.events]: \u001b[0m eta: 1 day, 13:47:04  iter: 2719  total_loss: 0.8507  loss_cls: 0.343  loss_box_reg: 0.3087  loss_mask: 0.1631  loss_rpn_cls: 0.004671  loss_rpn_loc: 0.01657    time: 6.1388  last_time: 6.4108  data_time: 0.0215  last_data_time: 0.0057   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:48:18 d2.utils.events]: \u001b[0m eta: 1 day, 13:47:21  iter: 2739  total_loss: 0.8262  loss_cls: 0.3265  loss_box_reg: 0.2956  loss_mask: 0.1561  loss_rpn_cls: 0.005603  loss_rpn_loc: 0.02109    time: 6.1384  last_time: 5.4556  data_time: 0.0285  last_data_time: 0.0057   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:50:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:18  iter: 2759  total_loss: 0.7552  loss_cls: 0.2945  loss_box_reg: 0.2806  loss_mask: 0.1418  loss_rpn_cls: 0.006479  loss_rpn_loc: 0.02159    time: 6.1381  last_time: 5.4658  data_time: 0.0281  last_data_time: 0.0067   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:52:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:22  iter: 2779  total_loss: 0.7258  loss_cls: 0.3072  loss_box_reg: 0.2683  loss_mask: 0.1459  loss_rpn_cls: 0.007063  loss_rpn_loc: 0.01676    time: 6.1397  last_time: 7.0718  data_time: 0.0393  last_data_time: 0.0055   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:54:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:20  iter: 2799  total_loss: 0.7108  loss_cls: 0.2554  loss_box_reg: 0.2614  loss_mask: 0.1406  loss_rpn_cls: 0.00312  loss_rpn_loc: 0.01573    time: 6.1383  last_time: 6.4351  data_time: 0.0186  last_data_time: 0.0078   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:56:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:39:39  iter: 2819  total_loss: 0.7585  loss_cls: 0.2655  loss_box_reg: 0.2729  loss_mask: 0.1342  loss_rpn_cls: 0.006745  loss_rpn_loc: 0.01697    time: 6.1394  last_time: 5.5773  data_time: 0.0181  last_data_time: 0.0062   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 04:58:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:38:36  iter: 2839  total_loss: 0.6826  loss_cls: 0.2747  loss_box_reg: 0.2827  loss_mask: 0.1311  loss_rpn_cls: 0.0042  loss_rpn_loc: 0.01519    time: 6.1398  last_time: 6.6900  data_time: 0.0220  last_data_time: 0.0951   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:00:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:28  iter: 2859  total_loss: 0.718  loss_cls: 0.2904  loss_box_reg: 0.2839  loss_mask: 0.1386  loss_rpn_cls: 0.004265  loss_rpn_loc: 0.0147    time: 6.1400  last_time: 7.5120  data_time: 0.0196  last_data_time: 0.0072   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:02:45 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:21  iter: 2879  total_loss: 0.7667  loss_cls: 0.3144  loss_box_reg: 0.273  loss_mask: 0.1409  loss_rpn_cls: 0.005932  loss_rpn_loc: 0.01603    time: 6.1411  last_time: 6.5695  data_time: 0.0414  last_data_time: 0.0348   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:04:47 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:32  iter: 2899  total_loss: 0.7168  loss_cls: 0.2952  loss_box_reg: 0.2651  loss_mask: 0.1378  loss_rpn_cls: 0.004741  loss_rpn_loc: 0.01407    time: 6.1406  last_time: 6.0009  data_time: 0.0234  last_data_time: 0.0813   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:06:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:39:07  iter: 2919  total_loss: 0.6937  loss_cls: 0.3175  loss_box_reg: 0.2377  loss_mask: 0.1279  loss_rpn_cls: 0.004197  loss_rpn_loc: 0.01278    time: 6.1404  last_time: 7.5525  data_time: 0.0387  last_data_time: 0.0047   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:08:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:52  iter: 2939  total_loss: 0.7327  loss_cls: 0.3084  loss_box_reg: 0.2992  loss_mask: 0.1457  loss_rpn_cls: 0.004763  loss_rpn_loc: 0.02136    time: 6.1396  last_time: 5.8456  data_time: 0.0289  last_data_time: 0.0945   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:10:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:37:23  iter: 2959  total_loss: 0.7361  loss_cls: 0.3057  loss_box_reg: 0.2675  loss_mask: 0.1334  loss_rpn_cls: 0.003166  loss_rpn_loc: 0.01525    time: 6.1395  last_time: 4.4384  data_time: 0.0162  last_data_time: 0.0050   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:12:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:38:26  iter: 2979  total_loss: 0.726  loss_cls: 0.2942  loss_box_reg: 0.2798  loss_mask: 0.143  loss_rpn_cls: 0.004951  loss_rpn_loc: 0.01368    time: 6.1401  last_time: 5.7948  data_time: 0.0177  last_data_time: 0.0072   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:15:02 d2.utils.events]: \u001b[0m eta: 1 day, 13:36:23  iter: 2999  total_loss: 0.7777  loss_cls: 0.3128  loss_box_reg: 0.3106  loss_mask: 0.158  loss_rpn_cls: 0.003806  loss_rpn_loc: 0.01678    time: 6.1406  last_time: 4.2897  data_time: 0.0274  last_data_time: 0.0058   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:16:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:04  iter: 3019  total_loss: 0.6669  loss_cls: 0.2841  loss_box_reg: 0.2548  loss_mask: 0.1297  loss_rpn_cls: 0.002465  loss_rpn_loc: 0.009987    time: 6.1376  last_time: 5.8660  data_time: 0.0215  last_data_time: 0.0058   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:18:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:15:54  iter: 3039  total_loss: 0.7157  loss_cls: 0.2692  loss_box_reg: 0.2679  loss_mask: 0.1378  loss_rpn_cls: 0.003463  loss_rpn_loc: 0.01991    time: 6.1346  last_time: 4.8251  data_time: 0.0304  last_data_time: 0.0071   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:20:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:16:10  iter: 3059  total_loss: 0.6749  loss_cls: 0.2737  loss_box_reg: 0.2522  loss_mask: 0.1244  loss_rpn_cls: 0.003275  loss_rpn_loc: 0.01408    time: 6.1339  last_time: 7.2480  data_time: 0.0236  last_data_time: 0.1263   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:22:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:15:07  iter: 3079  total_loss: 0.7383  loss_cls: 0.3234  loss_box_reg: 0.2437  loss_mask: 0.1374  loss_rpn_cls: 0.003056  loss_rpn_loc: 0.01602    time: 6.1346  last_time: 7.8853  data_time: 0.0387  last_data_time: 0.1092   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:25:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:05  iter: 3099  total_loss: 0.7224  loss_cls: 0.274  loss_box_reg: 0.2775  loss_mask: 0.1491  loss_rpn_cls: 0.002509  loss_rpn_loc: 0.01271    time: 6.1377  last_time: 7.0694  data_time: 0.0197  last_data_time: 0.0892   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:27:02 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:07  iter: 3119  total_loss: 0.7624  loss_cls: 0.3114  loss_box_reg: 0.2933  loss_mask: 0.1449  loss_rpn_cls: 0.007042  loss_rpn_loc: 0.01295    time: 6.1351  last_time: 4.0669  data_time: 0.0220  last_data_time: 0.0067   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:29:06 d2.utils.events]: \u001b[0m eta: 1 day, 13:15:25  iter: 3139  total_loss: 0.6962  loss_cls: 0.3031  loss_box_reg: 0.2663  loss_mask: 0.1497  loss_rpn_cls: 0.004048  loss_rpn_loc: 0.01256    time: 6.1357  last_time: 6.4217  data_time: 0.0276  last_data_time: 0.0045   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:31:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:11:45  iter: 3159  total_loss: 0.6977  loss_cls: 0.283  loss_box_reg: 0.2726  loss_mask: 0.1479  loss_rpn_cls: 0.003698  loss_rpn_loc: 0.01883    time: 6.1360  last_time: 6.8841  data_time: 0.0222  last_data_time: 0.0059   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:33:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:14:51  iter: 3179  total_loss: 0.7271  loss_cls: 0.2983  loss_box_reg: 0.2729  loss_mask: 0.1327  loss_rpn_cls: 0.003305  loss_rpn_loc: 0.02166    time: 6.1381  last_time: 6.0237  data_time: 0.0272  last_data_time: 0.0067   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:35:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:14:58  iter: 3199  total_loss: 0.7773  loss_cls: 0.2788  loss_box_reg: 0.2977  loss_mask: 0.1501  loss_rpn_cls: 0.005297  loss_rpn_loc: 0.02136    time: 6.1380  last_time: 5.2032  data_time: 0.0170  last_data_time: 0.0073   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:37:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:56  iter: 3219  total_loss: 0.7108  loss_cls: 0.2817  loss_box_reg: 0.2792  loss_mask: 0.1222  loss_rpn_cls: 0.001899  loss_rpn_loc: 0.01363    time: 6.1358  last_time: 6.5019  data_time: 0.0325  last_data_time: 0.2185   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:39:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:53  iter: 3239  total_loss: 0.7163  loss_cls: 0.2796  loss_box_reg: 0.2457  loss_mask: 0.1316  loss_rpn_cls: 0.003703  loss_rpn_loc: 0.01436    time: 6.1375  last_time: 7.2229  data_time: 0.0287  last_data_time: 0.0075   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:41:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:06:40  iter: 3259  total_loss: 0.7271  loss_cls: 0.2599  loss_box_reg: 0.2718  loss_mask: 0.1435  loss_rpn_cls: 0.002145  loss_rpn_loc: 0.0149    time: 6.1390  last_time: 7.9325  data_time: 0.0264  last_data_time: 0.0049   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:43:47 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:58  iter: 3279  total_loss: 0.7082  loss_cls: 0.2705  loss_box_reg: 0.2678  loss_mask: 0.1365  loss_rpn_cls: 0.004389  loss_rpn_loc: 0.01967    time: 6.1422  last_time: 5.2549  data_time: 0.0231  last_data_time: 0.0769   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:45:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:08:14  iter: 3299  total_loss: 0.6948  loss_cls: 0.2384  loss_box_reg: 0.2429  loss_mask: 0.1323  loss_rpn_cls: 0.00227  loss_rpn_loc: 0.01096    time: 6.1441  last_time: 6.7093  data_time: 0.0237  last_data_time: 0.0060   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:48:01 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:14  iter: 3319  total_loss: 0.6404  loss_cls: 0.3045  loss_box_reg: 0.2499  loss_mask: 0.1309  loss_rpn_cls: 0.002786  loss_rpn_loc: 0.01615    time: 6.1448  last_time: 6.3010  data_time: 0.0144  last_data_time: 0.0094   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:50:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:07  iter: 3339  total_loss: 0.6492  loss_cls: 0.2566  loss_box_reg: 0.2423  loss_mask: 0.1258  loss_rpn_cls: 0.003287  loss_rpn_loc: 0.01325    time: 6.1460  last_time: 6.5842  data_time: 0.0214  last_data_time: 0.0055   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:52:03 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:07  iter: 3359  total_loss: 0.7408  loss_cls: 0.2958  loss_box_reg: 0.2713  loss_mask: 0.1444  loss_rpn_cls: 0.005324  loss_rpn_loc: 0.01303    time: 6.1436  last_time: 3.5299  data_time: 0.0261  last_data_time: 0.0066   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:54:03 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:01  iter: 3379  total_loss: 0.6558  loss_cls: 0.2849  loss_box_reg: 0.2632  loss_mask: 0.1342  loss_rpn_cls: 0.002966  loss_rpn_loc: 0.01354    time: 6.1428  last_time: 5.1879  data_time: 0.0198  last_data_time: 0.0053   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:56:11 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:57  iter: 3399  total_loss: 0.752  loss_cls: 0.2622  loss_box_reg: 0.2812  loss_mask: 0.1469  loss_rpn_cls: 0.002966  loss_rpn_loc: 0.01575    time: 6.1441  last_time: 5.7909  data_time: 0.0231  last_data_time: 0.0062   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 05:58:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:58:54  iter: 3419  total_loss: 0.7167  loss_cls: 0.2893  loss_box_reg: 0.2731  loss_mask: 0.1178  loss_rpn_cls: 0.004506  loss_rpn_loc: 0.0177    time: 6.1440  last_time: 6.1187  data_time: 0.0391  last_data_time: 0.1393   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:00:13 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:42  iter: 3439  total_loss: 0.6892  loss_cls: 0.2411  loss_box_reg: 0.2594  loss_mask: 0.1425  loss_rpn_cls: 0.002922  loss_rpn_loc: 0.01514    time: 6.1432  last_time: 6.9520  data_time: 0.0282  last_data_time: 0.0043   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:02:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:01:33  iter: 3459  total_loss: 0.6448  loss_cls: 0.2501  loss_box_reg: 0.2623  loss_mask: 0.1318  loss_rpn_cls: 0.003513  loss_rpn_loc: 0.01607    time: 6.1440  last_time: 5.9815  data_time: 0.0130  last_data_time: 0.0076   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:04:30 d2.utils.events]: \u001b[0m eta: 1 day, 13:04:34  iter: 3479  total_loss: 0.7684  loss_cls: 0.2778  loss_box_reg: 0.2828  loss_mask: 0.1371  loss_rpn_cls: 0.005485  loss_rpn_loc: 0.02066    time: 6.1463  last_time: 6.1742  data_time: 0.0220  last_data_time: 0.0067   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:06:29 d2.utils.events]: \u001b[0m eta: 1 day, 12:57:34  iter: 3499  total_loss: 0.6674  loss_cls: 0.2832  loss_box_reg: 0.2606  loss_mask: 0.1298  loss_rpn_cls: 0.00227  loss_rpn_loc: 0.01266    time: 6.1450  last_time: 5.5531  data_time: 0.0228  last_data_time: 0.0036   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:08:40 d2.utils.events]: \u001b[0m eta: 1 day, 12:59:24  iter: 3519  total_loss: 0.679  loss_cls: 0.2743  loss_box_reg: 0.2432  loss_mask: 0.1298  loss_rpn_cls: 0.004285  loss_rpn_loc: 0.01452    time: 6.1474  last_time: 5.4090  data_time: 0.0399  last_data_time: 0.0459   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:10:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:55:56  iter: 3539  total_loss: 0.6923  loss_cls: 0.292  loss_box_reg: 0.2586  loss_mask: 0.1396  loss_rpn_cls: 0.002423  loss_rpn_loc: 0.01321    time: 6.1473  last_time: 6.5334  data_time: 0.0258  last_data_time: 0.0083   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:12:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:55:16  iter: 3559  total_loss: 0.6589  loss_cls: 0.2574  loss_box_reg: 0.2524  loss_mask: 0.1258  loss_rpn_cls: 0.002758  loss_rpn_loc: 0.01553    time: 6.1466  last_time: 6.3536  data_time: 0.0201  last_data_time: 0.0056   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:14:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:56:09  iter: 3579  total_loss: 0.7042  loss_cls: 0.2621  loss_box_reg: 0.2868  loss_mask: 0.1387  loss_rpn_cls: 0.004321  loss_rpn_loc: 0.01821    time: 6.1474  last_time: 7.1495  data_time: 0.0421  last_data_time: 0.0881   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:16:51 d2.utils.events]: \u001b[0m eta: 1 day, 12:48:02  iter: 3599  total_loss: 0.6396  loss_cls: 0.2805  loss_box_reg: 0.2491  loss_mask: 0.1262  loss_rpn_cls: 0.001944  loss_rpn_loc: 0.01286    time: 6.1469  last_time: 4.9016  data_time: 0.0172  last_data_time: 0.0047   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:18:54 d2.utils.events]: \u001b[0m eta: 1 day, 12:44:02  iter: 3619  total_loss: 0.6263  loss_cls: 0.2591  loss_box_reg: 0.2554  loss_mask: 0.1343  loss_rpn_cls: 0.004741  loss_rpn_loc: 0.01169    time: 6.1469  last_time: 6.4184  data_time: 0.0256  last_data_time: 0.0050   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:20:58 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:26  iter: 3639  total_loss: 0.7158  loss_cls: 0.2963  loss_box_reg: 0.2617  loss_mask: 0.1443  loss_rpn_cls: 0.004407  loss_rpn_loc: 0.01886    time: 6.1474  last_time: 4.7835  data_time: 0.0279  last_data_time: 0.0087   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:23:03 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:33  iter: 3659  total_loss: 0.6331  loss_cls: 0.2184  loss_box_reg: 0.2499  loss_mask: 0.1339  loss_rpn_cls: 0.004048  loss_rpn_loc: 0.01948    time: 6.1479  last_time: 4.1756  data_time: 0.0304  last_data_time: 0.0051   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:25:05 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:29  iter: 3679  total_loss: 0.6657  loss_cls: 0.2402  loss_box_reg: 0.2742  loss_mask: 0.1361  loss_rpn_cls: 0.002558  loss_rpn_loc: 0.01583    time: 6.1476  last_time: 5.6273  data_time: 0.0133  last_data_time: 0.0078   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:27:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:44:04  iter: 3699  total_loss: 0.6232  loss_cls: 0.2262  loss_box_reg: 0.2548  loss_mask: 0.1369  loss_rpn_cls: 0.003635  loss_rpn_loc: 0.01513    time: 6.1492  last_time: 7.0578  data_time: 0.0202  last_data_time: 0.0072   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:29:16 d2.utils.events]: \u001b[0m eta: 1 day, 12:41:40  iter: 3719  total_loss: 0.6678  loss_cls: 0.263  loss_box_reg: 0.2405  loss_mask: 0.1362  loss_rpn_cls: 0.001722  loss_rpn_loc: 0.01167    time: 6.1490  last_time: 7.3552  data_time: 0.0306  last_data_time: 0.1144   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:31:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:39:55  iter: 3739  total_loss: 0.7237  loss_cls: 0.2781  loss_box_reg: 0.2712  loss_mask: 0.1293  loss_rpn_cls: 0.003761  loss_rpn_loc: 0.01395    time: 6.1493  last_time: 7.1393  data_time: 0.0258  last_data_time: 0.0816   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:33:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:27  iter: 3759  total_loss: 0.6991  loss_cls: 0.29  loss_box_reg: 0.2432  loss_mask: 0.1311  loss_rpn_cls: 0.004165  loss_rpn_loc: 0.01415    time: 6.1502  last_time: 6.1675  data_time: 0.0320  last_data_time: 0.0152   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:35:21 d2.utils.events]: \u001b[0m eta: 1 day, 12:28:52  iter: 3779  total_loss: 0.6975  loss_cls: 0.2572  loss_box_reg: 0.2606  loss_mask: 0.139  loss_rpn_cls: 0.003591  loss_rpn_loc: 0.02041    time: 6.1479  last_time: 5.7785  data_time: 0.0194  last_data_time: 0.0046   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:37:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:25:37  iter: 3799  total_loss: 0.7144  loss_cls: 0.2877  loss_box_reg: 0.2548  loss_mask: 0.1385  loss_rpn_cls: 0.003663  loss_rpn_loc: 0.02115    time: 6.1487  last_time: 5.3178  data_time: 0.0218  last_data_time: 0.0053   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:39:29 d2.utils.events]: \u001b[0m eta: 1 day, 12:22:30  iter: 3819  total_loss: 0.6232  loss_cls: 0.2743  loss_box_reg: 0.2244  loss_mask: 0.1268  loss_rpn_cls: 0.003731  loss_rpn_loc: 0.01672    time: 6.1484  last_time: 7.9823  data_time: 0.0183  last_data_time: 0.0057   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:41:34 d2.utils.events]: \u001b[0m eta: 1 day, 12:20:27  iter: 3839  total_loss: 0.6294  loss_cls: 0.2677  loss_box_reg: 0.2323  loss_mask: 0.1202  loss_rpn_cls: 0.002598  loss_rpn_loc: 0.01706    time: 6.1490  last_time: 7.3034  data_time: 0.0306  last_data_time: 0.0052   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:43:34 d2.utils.events]: \u001b[0m eta: 1 day, 12:17:26  iter: 3859  total_loss: 0.6737  loss_cls: 0.2605  loss_box_reg: 0.2542  loss_mask: 0.136  loss_rpn_cls: 0.00251  loss_rpn_loc: 0.01503    time: 6.1483  last_time: 6.8532  data_time: 0.0257  last_data_time: 0.0072   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:45:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:12:22  iter: 3879  total_loss: 0.665  loss_cls: 0.266  loss_box_reg: 0.2616  loss_mask: 0.1444  loss_rpn_cls: 0.003207  loss_rpn_loc: 0.01437    time: 6.1491  last_time: 5.2676  data_time: 0.0454  last_data_time: 0.0367   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:47:44 d2.utils.events]: \u001b[0m eta: 1 day, 12:10:55  iter: 3899  total_loss: 0.6327  loss_cls: 0.2605  loss_box_reg: 0.2408  loss_mask: 0.1303  loss_rpn_cls: 0.002485  loss_rpn_loc: 0.01636    time: 6.1492  last_time: 6.4641  data_time: 0.0171  last_data_time: 0.1219   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:49:44 d2.utils.events]: \u001b[0m eta: 1 day, 12:07:55  iter: 3919  total_loss: 0.6665  loss_cls: 0.2545  loss_box_reg: 0.2381  loss_mask: 0.1397  loss_rpn_cls: 0.001869  loss_rpn_loc: 0.01228    time: 6.1484  last_time: 6.4850  data_time: 0.0152  last_data_time: 0.0050   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:51:39 d2.utils.events]: \u001b[0m eta: 1 day, 12:04:47  iter: 3939  total_loss: 0.6617  loss_cls: 0.2426  loss_box_reg: 0.2458  loss_mask: 0.1309  loss_rpn_cls: 0.001835  loss_rpn_loc: 0.01631    time: 6.1464  last_time: 5.6316  data_time: 0.0288  last_data_time: 0.0056   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:53:40 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:43  iter: 3959  total_loss: 0.6285  loss_cls: 0.2444  loss_box_reg: 0.2387  loss_mask: 0.124  loss_rpn_cls: 0.002665  loss_rpn_loc: 0.01562    time: 6.1459  last_time: 7.1454  data_time: 0.0312  last_data_time: 0.0081   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:55:46 d2.utils.events]: \u001b[0m eta: 1 day, 11:57:19  iter: 3979  total_loss: 0.6321  loss_cls: 0.2514  loss_box_reg: 0.265  loss_mask: 0.1224  loss_rpn_cls: 0.003986  loss_rpn_loc: 0.01291    time: 6.1466  last_time: 5.3512  data_time: 0.0235  last_data_time: 0.1045   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 06:57:55 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:37  iter: 3999  total_loss: 0.6504  loss_cls: 0.2556  loss_box_reg: 0.2489  loss_mask: 0.1272  loss_rpn_cls: 0.001916  loss_rpn_loc: 0.01614    time: 6.1481  last_time: 6.6105  data_time: 0.0280  last_data_time: 0.0043   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:00:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:56:52  iter: 4019  total_loss: 0.7312  loss_cls: 0.2373  loss_box_reg: 0.2667  loss_mask: 0.1422  loss_rpn_cls: 0.004258  loss_rpn_loc: 0.01639    time: 6.1490  last_time: 7.5689  data_time: 0.0215  last_data_time: 0.0068   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:02:11 d2.utils.events]: \u001b[0m eta: 1 day, 11:56:31  iter: 4039  total_loss: 0.6553  loss_cls: 0.2452  loss_box_reg: 0.2391  loss_mask: 0.1276  loss_rpn_cls: 0.003808  loss_rpn_loc: 0.01826    time: 6.1505  last_time: 5.3119  data_time: 0.0288  last_data_time: 0.0070   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:04:09 d2.utils.events]: \u001b[0m eta: 1 day, 11:53:51  iter: 4059  total_loss: 0.6487  loss_cls: 0.2608  loss_box_reg: 0.2266  loss_mask: 0.1194  loss_rpn_cls: 0.002523  loss_rpn_loc: 0.01445    time: 6.1492  last_time: 5.8148  data_time: 0.0231  last_data_time: 0.1138   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:06:13 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:44  iter: 4079  total_loss: 0.7157  loss_cls: 0.2975  loss_box_reg: 0.2417  loss_mask: 0.1404  loss_rpn_cls: 0.002652  loss_rpn_loc: 0.02049    time: 6.1495  last_time: 5.9300  data_time: 0.0227  last_data_time: 0.1334   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:08:15 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:44  iter: 4099  total_loss: 0.6035  loss_cls: 0.2552  loss_box_reg: 0.2259  loss_mask: 0.125  loss_rpn_cls: 0.001267  loss_rpn_loc: 0.009652    time: 6.1494  last_time: 5.8985  data_time: 0.0225  last_data_time: 0.0056   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:10:24 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:25  iter: 4119  total_loss: 0.6135  loss_cls: 0.244  loss_box_reg: 0.2382  loss_mask: 0.117  loss_rpn_cls: 0.00359  loss_rpn_loc: 0.01506    time: 6.1506  last_time: 6.3382  data_time: 0.0260  last_data_time: 0.0050   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:12:32 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:34  iter: 4139  total_loss: 0.6703  loss_cls: 0.2633  loss_box_reg: 0.2436  loss_mask: 0.1306  loss_rpn_cls: 0.003438  loss_rpn_loc: 0.01693    time: 6.1519  last_time: 8.3347  data_time: 0.0419  last_data_time: 0.0062   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:14:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:32  iter: 4159  total_loss: 0.6333  loss_cls: 0.223  loss_box_reg: 0.2435  loss_mask: 0.1198  loss_rpn_cls: 0.001981  loss_rpn_loc: 0.01074    time: 6.1519  last_time: 5.5085  data_time: 0.0249  last_data_time: 0.0028   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:16:38 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:29  iter: 4179  total_loss: 0.6245  loss_cls: 0.211  loss_box_reg: 0.2502  loss_mask: 0.1296  loss_rpn_cls: 0.002637  loss_rpn_loc: 0.01161    time: 6.1519  last_time: 5.5739  data_time: 0.0287  last_data_time: 0.0083   lr: 0.001  max_mem: 8233M\n",
            "\u001b[32m[08/09 07:18:38 d2.utils.events]: \u001b[0m eta: 1 day, 11:41:57  iter: 4199  total_loss: 0.682  loss_cls: 0.2707  loss_box_reg: 0.2355  loss_mask: 0.1266  loss_rpn_cls: 0.003829  loss_rpn_loc: 0.01837    time: 6.1511  last_time: 5.9676  data_time: 0.0216  last_data_time: 0.0042   lr: 0.001  max_mem: 8233M\n"
          ]
        }
      ],
      "source": [
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flInE1L-XTfx"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vsByFDFbQwLi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[08/08 14:53:32 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from ./v1-train/mask_rcnn_R_50_FPN_3x/2023-08-08-13-25-14/model_final.pth ...\n"
          ]
        }
      ],
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(\"./v1-train/mask_rcnn_R_50_FPN_3x/2023-08-08-13-25-14/\", \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hmAcBbpXX-Rh",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[08/08 14:53:33 d2.data.datasets.coco]: \u001b[0m\n",
            "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
            "\n",
            "\u001b[32m[08/08 14:53:33 d2.data.datasets.coco]: \u001b[0mLoaded 71 images in COCO format from ./datasets/v1/valid/_annotations.coco.json\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visualized images and results saved in ./results\\output15\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Determine the next output folder number\n",
        "results_root = \"./results\"\n",
        "output_folder_count = 1\n",
        "while os.path.exists(os.path.join(results_root, f\"output{output_folder_count}\")):\n",
        "    output_folder_count += 1\n",
        "\n",
        "# Create a new output folder for this run\n",
        "output_dir = os.path.join(results_root, f\"output{output_folder_count}\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "dataset_valid = DatasetCatalog.get(\"Z:/Uni/Tese/Code/TESTES/\")\n",
        "# Load the model's metadata\n",
        "metadata = MetadataCatalog.get(\"Z:/Uni/Tese/Code/TESTES/\")\n",
        "\n",
        "# Create a text file to save results\n",
        "txt_filename = os.path.join(output_dir, \"results.txt\")\n",
        "\n",
        "with open(txt_filename, \"w\") as txt_file:\n",
        "    for idx, d in enumerate(dataset_valid):\n",
        "        img = cv2.imread(d[\"file_name\"])\n",
        "        outputs = predictor(img)\n",
        "        \n",
        "        # Map class IDs to class names\n",
        "        class_names = [metadata.thing_classes[i] for i in outputs[\"instances\"].pred_classes]\n",
        "        \n",
        "        visualizer = Visualizer(\n",
        "            img[:, :, ::-1],\n",
        "            metadata=metadata,  # Pass the metadata for proper class name mapping\n",
        "            scale=0.8, \n",
        "            instance_mode=ColorMode.IMAGE\n",
        "        )\n",
        "        \n",
        "        # Draw instance predictions with class names\n",
        "        out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "        \n",
        "        output_filename = os.path.basename(d[\"file_name\"])  # Get the original image name\n",
        "        output_filename = os.path.splitext(output_filename)[0]  # Remove extension\n",
        "        output_filename = f\"{output_filename}_output_{idx}.jpg\"\n",
        "        output_filename = os.path.join(output_dir, output_filename)\n",
        "        \n",
        "        cv2.imwrite(output_filename, out.get_image()[:, :, ::-1])\n",
        "        \n",
        "        # Write results to the text file\n",
        "        txt_file.write(f\"Image name: {d['file_name']}\\n\")\n",
        "        txt_file.write(\"Predictions: \")\n",
        "\n",
        "        # Get the predicted class indices\n",
        "        pred_classes = outputs[\"instances\"].pred_classes.to(\"cpu\").numpy()\n",
        "\n",
        "        # Calculate the count of each predicted class\n",
        "        class_counts = np.bincount(pred_classes, minlength=len(metadata.thing_classes))\n",
        "\n",
        "        # Filter out classes with non-zero counts and get their corresponding names\n",
        "        non_zero_classes = [(class_name, count) for class_name, count in zip(metadata.thing_classes, class_counts) if count > 0]\n",
        "\n",
        "        # Write the non-zero class predictions to the text file\n",
        "        predictions = [f\"{count} {class_name}\" for class_name, count in non_zero_classes]\n",
        "        txt_file.write(\", \".join(predictions))\n",
        "        txt_file.write(\"\\n\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Visualized images and results saved in\", output_dir)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
